<!DOCTYPE html >
<html lang="en-US">

  <!-- Site Head -->
<head>
  <meta charset="utf-8" />
  <title>Unfolding the multiscale structure of networks with dynamical Ollivier-Ricci curvature</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" href="/images/logo.png" title="Favicon" />
  <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">

  <!-- Importing Styles -->
  <link rel="stylesheet" href="/css/style.css">
</head>


  <body>

    <!-- Page Loader -->
    <div id="page-loader" class="page-loader"><span class="load-icon"></span></div>

    <!-- Site Header -->
<header class="site-header blog-header parallax" id="home" data-scrollax-parent="true">

  <!-- Parallax Header Image -->
  <div class="parallax-wrapper">
    <div class="cover" data-scrollax="properties: { 'translateY': '48%' }">
      <img src="/images/header-image.jpg" />
    </div>
  </div>

  <div class="container">

    <!-- Site Navigation Bar -->
    <div class="row site-navigation-bar parent">

      <!-- Logo -->
      <div class="col-sm-2 col-xs-4 logo child-center-v">
        <a href="/index.html"><img src="/images/logo.svg" width="100"
            alt="Dynamics of Neural Systems Lab Logo"></a>
      </div>

      <!-- Toggle Mobile Menu -->
      <div class="col-xs-8 toggle-menu child-center-v">

        <!-- Burger -->
        <div class="navTrigger">
          <i></i><i></i><i></i>
        </div>

      </div>

      <!-- Navigation Bar -->
      <div class="col-sm-10 col-xs-12 navigation-bar child-center-v">

        <!-- Navigation Menu -->
        <nav id="navMenu" class="nav-menu">
          <ul class="menu">
            <li><a href="/index.html#home">Home</a></li>
            <li><a href="/index.html#about">About</a></li>
            <li><a href="/index.html#research">Research</a></li>
            <li><a href="/index.html#team">Team</a></li>
            <li><a href="/index.html#news">News</a></li>
          </ul>
        </nav>

      </div>

    </div>

    <!-- Site Header Banner -->
    <div class="row header-banner parent" data-scrollax="properties: { 'translateY': '48%', 'opacity': '1.4' }">

      <div class="heading-content text-center child-center-v">

        <!-- Heading Title -->
        <div class="row heading-title">
          <h2>
            
            
            
          </h2>
          <h2 class="thin">Unfolding the multiscale structure of networks with dynamical Ollivier-Ricci curvature</h2>
        </div>

      </div>

    </div>

    <!-- Arrow Down -->
    <div class="row arrow-down-section" data-scrollax="properties: { 'opacity': '0.6' }">

      <div class="wrapper-arrow child-center">
        <a href="#start" class="arrow-down">
          <i class="pe-7s-angle-down"></i>
        </a>
      </div>

    </div>

  </div>
</header>


    <!-- Page Section -->
    <section class="site-section blog-section page-layout" id="start">
      <div class="container">

        <!-- Page Layout -->
        <div class="row">

          <!-- Content -->
          <div class="col-md-8 blog-articles-container">

            <h2 id="understanding-networks-by-their-shape">Understanding Networks by Their Shape</h2>

<p>Networks are omnipresent, but not all networks are easily conceptualized in the physical world. Some networks, like our nervous system of interconnected neurons, railway lines, and electrical power grids, are physical objects embedded in 2-dimensional or 3-dimensional physical space, where the nodes represent points in space. On the other hand, networks such as economic networks and social networks are abstract objects representing interactions between entities. For some of these networks, a physical intuition is still possible. For instance, nodes may contain a list of features, and edges are drawn based on the relationship between these features. However, the dimension of this space (number of features) can be large, making interpretability difficult. In general, the majority of networks, like the World Wide Web or biochemical networks inside living cells, come with no intrinsic notion of space. Nodes have no spatial coordinates or features to facilitate physical intuition; the network represents only the abstraction of a set of entities with some connections between them.</p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-03-at-16.07.45.png?w=1024" alt="Networks with edges coloured by their dynamic Ollivier-Ricci curvature" />
<em>Networks with edges coloured by their dynamic Ollivier-Ricci curvature at two different resolutions. Nodes represent clusters based on maximising the average edge curvature within a cluster.</em></p>

<h2 id="understanding-networks-via-embeddings">Understanding Networks via Embeddings</h2>

<p>Having a space in which the network can be laid out, a so-called <a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture">embedding space</a> turns out to be very useful for analyzing networks. For example, assuming that the network’s nodes follow a continuous structure (manifold approximation), connections respecting this continuity can be preserved while those violating it may be ignored. This assumption allows unfolding the network in a 2D embedding space, often very useful to reveal its structure. Another example relates to <a href="https://en.wikipedia.org/wiki/Complex_network">complex networks</a>, which obey <a href="https://en.wikipedia.org/wiki/Power_law">power-law</a> <a href="https://en.wikipedia.org/wiki/Degree_distribution">degree distributions</a>. <a href="https://www.nature.com/articles/s41567-018-0072-5">García-Pérez and colleagues</a> found that nodes in complex networks can be naturally embedded in <a href="https://en.wikipedia.org/wiki/Hyperbolic_space">hyperbolic space</a>, known from M.C. Escher drawings. Remarkably, when nodes are clustered based on proximity in hyperbolic space, the community structure is preserved—meaning the network structure is invariant under hyperbolic geometry. Without an embedding space, there is no inherent notion of direction in a network, making it harder to understand the spreading of information. Finding an embedding space may also be used to study the spreading of dynamical processes in networks, such as epidemics. <a href="https://science.sciencemag.org/content/342/6164/1337">Helbing and colleagues</a> studied the spreading of infectious diseases on networks by defining an effective distance that maps nodes into a space where the wavefront started at a given node reaches points at the same time at a given radial distance. Using this distance to map the network into 2D space revealed node communities in which the infection started at a given point in the network arrives at the same time. Such information is useful for controlling the infectious process by blocking targeted edges.</p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-03-at-15.40.03-1.png?w=698" alt="Manifold approximation (Tennenbaum et al., 2000)" />
<em>Manifold approximation (Tennenbaum et al., 2000)</em></p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-03-at-15.40.10-2.png?w=872" alt="Hyperbolic space embedding (Garcia-Perez et al., 2018)" />
<em>Hyperbolic space embedding (Garcia-Perez et al., 2018)</em></p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-03-at-15.40.31-1.png?w=846" alt="Dynamic embedding (Helbing et al., 2013)" />
<em>Dynamic embedding (Helbing et al., 2013)</em></p>

<p>While embeddings are useful, they also have their dangers. In fact, for many networks, embedding their nodes into any <a href="https://en.wikipedia.org/wiki/Normed_vector_space">normed vector space</a> will necessarily lose much of the information encoded in the connectivity. The process of embedding stretches and shrinks the edges between pairs of edges by amounts comparable to the length of the edge itself. Many techniques in the field of <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">non-linear dimensionality reduction</a> deal with cleverly ‘distributing’ these distortions to preserve relevant features of the data while losing others. For example, the <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a> method aims at preserving the distance between nearby points while allowing points far away to be embedded arbitrarily. However, it is currently not well understood what features of the network one should preserve to create meaningful embeddings.</p>

<h2 id="geometry-of-networks">Geometry of Networks</h2>

<p>An alternative to using embeddings could be to define the geometry of the network based on analogues borrowed from <a href="https://en.wikipedia.org/wiki/Differential_geometry">differential geometry</a>, which deals with the geometry of continuous spaces. One way to define geometry in continuous spaces is through the concept of <a href="https://en.wikipedia.org/wiki/Curvature">curvature</a>. Consider two points <em>x</em> and <em>y</em> (on a manifold) and move them in parallel in an arbitrary direction to points <em>x’</em> and <em>y’</em>. Depending on the geometry of the space, the transported points will be closer or further away. By computing the average distance of points nearby <em>x</em> and points nearby <em>y</em> relative to the distance of <em>x</em> and <em>y</em>, one can define the <a href="https://en.wikipedia.org/wiki/Ricci_curvature">Ricci curvature</a> in direction <em>xy</em>.</p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-03-at-15.59.09.png?w=300" alt="Consider two points *x*, *y* in a space. Then move *x* a small distance in a direction given by the white arrow. Likewise, move *y* in the same direction (known as parallel transport). Depending on the curvature of the space, the transported points are on average closer or further." />
<em>Consider two points *x</em>, <em>y</em> in a space. Then move <em>x</em> a small distance in a direction given by the white arrow. Likewise, move <em>y</em> in the same direction (known as <a href="https://en.wikipedia.org/wiki/Parallel_transport">parallel transport</a>). Depending on the curvature of the space, the transported points are on average closer or further.*</p>

<p>Generalizing the notion of Ricci curvature to networks is less than straightforward. Because curvature requires a notion of direction, which is lacking in networks, there is no unique way to define network geometry. It is therefore not clear which one will yield insights into the structure of the network. In this work, we studied the notion developed by Ollivier, which, ‘simply’ put, replaces the average distance between points <em>x’</em> and <em>y’</em> with the <a href="https://en.wikipedia.org/wiki/Transportation_theory_(mathematics)">optimal transport distance</a>. The optimal transport or Wasserstein distance represents the least energy required to move a pile of sand to a pit. The distribution on the neighbors <em>x’</em> can also be thought of as units of sand (to be transported), while the distribution on the neighbors <em>y’</em> as unit holes (to be filled in). Intuitively, when <em>x</em> and <em>y</em> share neighbors, the holes are already filled in without moving mass. Thus, the higher the number of shared neighbors, the lower the transportation distance and the higher the curvature.</p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-03-at-17.16.49-5.png?w=300" alt="The Ollivier-Ricci curvature is defined by generalizing the Ricci curvature by replacing the average distance of points *x'* and *y'* by the optimal transport distance." />
<em>The Ollivier-Ricci curvature is defined by generalizing the Ricci curvature by replacing the average distance of points *x’</em> and <em>y’</em> by the optimal transport distance.*</p>

<p>When one computes the Ollivier-Ricci curvature only between adjacent nodes on the network (endpoints of an edge), it turns out to be remarkably intuitive. There is a deep analogy between the curvature of canonical networks and canonical geometric spaces, as illustrated by the figure below.</p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-03-at-17.27.28.png?w=1024" alt="Correspondence between the Ollivier-Ricci curvature (top) and Ricci curvature (bottom) on canonical geometries." />
<em>Correspondence between the Ollivier-Ricci curvature (top) and Ricci curvature (bottom) on canonical geometries.</em></p>

<h2 id="dynamic-ollivier-ricci-curvature">Dynamic Ollivier-Ricci curvature</h2>

<p>Our idea was to replace the notion of the neighborhood of points <em>x</em>, <em>y</em> by <a href="https://en.wikipedia.org/wiki/Diffusion_process">diffusion processes</a> started at points <em>x</em>, <em>y</em>. The diffusion processes distribute mass unevenly and on all nodes, rather than just the neighbors, which makes the problem more interesting. In the following figure, the red lines show the edge connecting two points, and the orange and blue semi-circles show the evolving diffusions. One can see that the diffusions stay trapped in well-connected regions of the network for some time before spreading to the rest of the network. In Ollivier’s definition, the distance between neighboring points is equally weighted because all neighbors carry the same mass. By considering diffusions, we place different importance on different pairs of points, i.e., different amounts of mass are placed over nodes as given by the size of semi-circles. Moreover, as the diffusions propagate, the dynamic geometry captures increasingly coarser features of the network. Therefore, instead of a single geometric object (like in classical Ollivier-Ricci curvature), we obtain a family of geometric objects parametrized by the time of diffusion.</p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/ezgif-1-b291722e3f33-1.gif?w=500" alt="Propagation of a pair of diffusions started at adjacent nodes in different clusters" />
<em>Propagation of a pair of diffusions started at adjacent nodes in different clusters</em></p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/ezgif-1-3ce516e58b0a-1.gif?w=500" alt="Propagation of a pair of diffusions started at adjacent nodes in the same clusters" />
<em>Propagation of a pair of diffusions started at adjacent nodes in the same clusters</em></p>

<h2 id="curvature-gaps-and-information-propagation">Curvature gaps and information propagation</h2>

<p>Because of the different timescales on which pairs of diffusions spread in the network, one may wonder how this could be reflected in the curvature of the network. One way to answer this is to visualize the curvature evolution, i.e., plotting the curvature value of each edge <em>ij</em> at a given point in time τ that marks the state of the diffusions. An observation is that the curvature distribution splits up into ‘bundles’ as time progresses. This observation makes sense by looking at the small networks in the insets. When both diffusions start within clusters (well-connected network regions), they become quickly overlapping, thus increasing the curvature. Meanwhile, for other edges spanning different clusters, the diffusions tend to overlap less and thus the curvature increases more slowly. One can now realize that, eventually, the diffusions started from anywhere will fully overlap, which will cause the curvatures to increase to the value of one as time goes to infinity. However, interesting structures are found at smaller times when the network diffusions are still far from being fully dispersed.</p>

<p>In our <a href="http://rdcu.be/cp3od">work</a>, we found that the differences in the curvatures of bundles can be related to the rate of information exchange between diffusions. This is mathematically characterized by the rate of <a href="https://en.wikipedia.org/wiki/Mixing_(mathematics)">mixing</a> of diffusions. More precisely, when the curvature reaches 0.75, as shown by the red dashed line, the corresponding pair of diffusions have mixed well. Note, they have not fully dispersed yet (they are not at stationarity). Instead, from this point in time onwards, they will move together until the dispersed state. Looking at the points in time where some diffusion pairs have mixed, shown by the vertical dashes, one can find edges that have still a long way until mixing. We call these curvature gaps that indicate bottlenecks in information flow. These bottlenecks can be visualized too! In the middle figure, we see that the curvature is lower between clusters than within clusters. In the right picture, the curvature is similar for edges within clusters and across the top and bottom clusters, while lower for those between the left and right clusters. These patterns reflect different scales in the graph revealed completely geometrically!</p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-03-at-17.57.19.png?w=1024" alt="Curvature distribution at different times of the diffusion processes. Initially, all curvatures are 0. As time progresses, the distribution separates into 'bundles' and develops gaps. When the curvature reaches 0.75, the diffusions are well-mixed between regions spanned by the given edge. Looking at the curvature distribution at timepoints where the individual bundles reach 0.75 visually reveals multiple clusters based on the sign of the curvature." />
<em>Curvature distribution at different times of the diffusion processes. Initially, all curvatures are 0. As time progresses, the distribution separates into ‘bundles’ and develops gaps. When the curvature reaches 0.75, the diffusions are well-mixed between regions spanned by the given edge. Looking at the curvature distribution at timepoints where the individual bundles reach 0.75 visually reveals multiple clusters based on the sign of the curvature.</em></p>

<p>To take this idea further, we looked for ways to modify the information flow between clusters. We accomplished this in two ways. First, we changed the ratio between the number of edges between clusters to that within clusters, called the edge density ratio <em>r</em>. A low edge density ratio means that it is hard for the diffusions to pass from one cluster to another, essentially creating more contrast between the communities. Second, but more subtle, we reduced the number of edges in the graph while maintaining the edge density ratio. This has the effect of reducing the mean degree <em>k</em>, the average number of edges connected to each node. The figure below illustrates on a graph with two clusters that as we remove more edges the communities become less distinguishable.</p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/ezgif-3-e4027fb93754.gif?w=500" alt="(right) As the network becomes sparser, there is less information about the community structure. (left) Initially, the node degrees are evenly distributed around a mean (~60), which becomes disordered with many low degree and a few high degree nodes." />
<em>(right) As the network becomes sparser, there is less information about the community structure. (left) Initially, the node degrees are evenly distributed around a mean (~60), which becomes disordered with many low degree and a few high degree nodes.</em></p>

<p>We then simulated many such graphs for different mean degrees (<em>k</em>) and edge density ratios and computed the curvature gap. From the following figure, it is clear that when the mean degree is lower, a lower edge density ratio suffices to establish a curvature gap. This observation makes sense: the fewer edges the graph has, the larger the contrast needs to be to find communities reliably. As the edge density ratio increases, the curvature gap is no longer different from background noise, shown by the black horizontal line. What is more surprising is that the point at which the curvature gap disappears for a given average degree is the final point when the edges can convey any information about the community structure. In fact, beyond this point, known as the Kesten-Stigum bound, there is no efficient algorithm that can tell the communities apart. When we look at the distribution of the transition points for different mean degrees, the prediction of the curvature gap lies very close to the theoretical prediction! In the <a href="http://rdcu.be/cp3od">paper</a>, we show that this is no coincidence; in</p>

<h2 id="multiscale-network-clustering-using-geometry">Multiscale network clustering using geometry</h2>

<p>Because of the strong link between curvature gap and network communities, one natural application of our theory is <a href="https://en.wikipedia.org/wiki/Community_structure">network clustering</a> or community detection. The nodes of a network can be grouped into communities based on different criteria, so the ‘correctness’ of communities is in general a subjective matter. However, clustering networks based on finding curvature gaps, i.e., directions of limited information flow is a valuable viewpoint. Firstly, because the curvature distribution encodes all information about the communities (disclaimer: our results show this for <a href="https://en.wikipedia.org/wiki/Stochastic_block_model">stochastic block model</a> graphs), clustering using curvature performs substantially better than classical node-clustering methods such as <a href="https://en.wikipedia.org/wiki/Spectral_clustering">spectral clustering</a>. Secondly, because of the association of low curvature edges with bottlenecks, communities are interpreted as containing nodes that share redundant information and that easily disconnect from the rest of the network. In other words, breaking low curvature edges is expected to have the largest effect on the information spreading. </p>

<p>To show that our framework can truly find useful applications, we clustered two real-world networks: the European power grid network and a network of <a href="https://en.wikipedia.org/wiki/Caenorhabditis_elegans">C. elegans</a> neurons constructed based on the similarity of their <a href="https://en.wikipedia.org/wiki/Homeobox">homeobox</a> gene expressions (homeobox genes are genes expressing proteins called transcription factors). Sparing the details for the interested reader, we could reveal striking information in both networks. Clustering the European power grid revealed clusters representing countries and other geographical regions on one scale as well as clusters representing the historical split between Eastern and Western Europe on another scale. All this information from nothing more than the wiring of electrical power lines! Not less interesting is the <a href="https://en.wikipedia.org/wiki/Caenorhabditis_elegans">C. elegans</a> gene expression network. Clustering this network revealed the anatomical type of most of the 302 neurons in the worm! In the future, we hope to find more links between the communities identified from the curvature distribution to physical or biological properties of real-world networks.</p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/untitled-2.jpg" alt="European power grid network and C. elegans neurons clustering" />
<em>European power grid network and C. elegans neurons clustering</em></p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-04-at-17.44.48-1.png" alt="C. elegans neurons clustering" />
<em>C. elegans neurons clustering</em></p>

<p><img src="https://adamgosztolai.files.wordpress.com/2021/08/untitled-3.jpg" alt="C. elegans neurons clustering" />
<em>C. elegans neurons clustering</em></p>

<hr />

<h2 id="references">References</h2>

<p>Gosztolai, A., Arnaudon, A. Unfolding the multiscale structure of networks with dynamical Ollivier-Ricci curvature. <a href="https://rdcu.be/cp3od">Nature Communications 12, 4561 (2021)</a> <a href="https://github.com/agosztolai/geometric_clustering">code</a></p>


          </div>

        </div>

      </div>
    </section>

    <!-- Site Footer -->
<footer class="site-footer" id="footer">
  <div class="container">

    <div class="row">
      <div class="col-md-6 col-md-push-3 footer-info text-center">

        <div class="social-icons">
          <ul class="icons">
            <!-- <li><a href="" target="_blank"><i class="fi flaticon-youtube"></i></a></li> -->
            <li><a href="https://twitter.com/AGosztolai" target="_blank"><i class="fa-brands fa-square-x-twitter"></i></a></li>
            <li><a href="https://scholar.google.com/citations?user=fVGCjMsAAAAJ&hl=en" target="_blank"><i class="fa-brands fa-google-scholar"></i></a></li>
          </ul>
        </div>

      </div>

    </div>

  </div>
</footer>

<!-- Go To Top Button -->
<a href="#home" class="go-to-top-button"><i class="pe-7s-angle-up"></i></a>


    <!-- Importing Scripts -->

<!-- Fontawesome -->
<script src="https://kit.fontawesome.com/590872f71d.js" crossorigin="anonymous"></script>

<!-- jQuery CDN -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/3.0.0/jquery.min.js"></script>

<!-- If CDN fails than use local jQuery -->
<script>window.jQuery || document.write('<script src="/js/jquery-3.0.0.min.js"><\/script>')</script>

<!-- Owl Carousel -->
<script src="/js/carousel.min.js" charset="utf-8"></script>

<!-- Parallaxing & SmoothScroll -->
<script src="/js/scrollax.min.js" charset="utf-8"></script>



<!-- Main JS -->
<script src="/js/main.js" charset="utf-8"></script>

<script>

document.addEventListener("DOMContentLoaded", function() {
  let paths = document.querySelectorAll('.lines path');
  let polygonsGroup = document.querySelector('.polygons');
  let svgContainer = document.querySelector('.line-container');
  let completedAnimation = false;
  let animationDuration = 3000; // Animation duration in milliseconds
  let animationStartTime;
  let targetTime = animationDuration; // Target time for animation completion
  let animationDelay = 2000; // Delay in milliseconds before animation starts

  polygonsGroup.style.opacity = '0';

  paths.forEach((path) => {
    let pathLength = path.getTotalLength();
    path.style.strokeDasharray = pathLength + ' ' + pathLength;
    path.style.strokeDashoffset = pathLength;
  });

  function startAnimation() {
    animationStartTime = performance.now(); // Get the current timestamp
    requestAnimationFrame(animate);
  }

  function animate(currentTime) {
    let elapsed = currentTime - animationStartTime;
    let animationProgress = Math.min(elapsed / animationDuration, 1);

    paths.forEach((path) => {
      let pathLength = path.getTotalLength();
      let drawLength = pathLength * (1 - animationProgress);
      path.style.strokeDashoffset = drawLength;
    });

    if (animationProgress < 1) {
      requestAnimationFrame(animate);
    } else {
      polygonsGroup.style.opacity = '1';
    }
  }

  setTimeout(startAnimation, animationDelay); // Start the animation after a delay
});

  </script>


  </body>

</html>

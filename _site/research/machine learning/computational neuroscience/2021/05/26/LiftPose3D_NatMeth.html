<!DOCTYPE html >
<html lang="en-US">

  <!-- Site Head -->
<head>
  <meta charset="utf-8" />
  <title>LiftPose3D, a deep learning-based approach for transforming two-dimensional to three-dimensional poses in laboratory animals</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" href="/images/logo.png" title="Favicon" />
  <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">

  <!-- Importing Styles -->
  <link rel="stylesheet" href="/css/style.css">
</head>


  <body>

    <!-- Page Loader -->
    <div id="page-loader" class="page-loader"><span class="load-icon"></span></div>

    <!-- Site Header -->
<header class="site-header blog-header parallax" id="home" data-scrollax-parent="true">

  <!-- Parallax Header Image -->
  <div class="parallax-wrapper">
    <div class="cover" data-scrollax="properties: { 'translateY': '48%' }">
      <img src="/images/header-image.jpg" />
    </div>
  </div>

  <div class="container">

    <!-- Site Navigation Bar -->
    <div class="row site-navigation-bar parent">

      <!-- Logo -->
      <div class="col-sm-2 col-xs-4 logo child-center-v">
        <a href="/index.html"><img src="/images/logo.svg" width="100"
            alt="Dynamics of Neural Systems Lab Logo"></a>
      </div>

      <!-- Toggle Mobile Menu -->
      <div class="col-xs-8 toggle-menu child-center-v">

        <!-- Burger -->
        <div class="navTrigger">
          <i></i><i></i><i></i>
        </div>

      </div>

      <!-- Navigation Bar -->
      <div class="col-sm-10 col-xs-12 navigation-bar child-center-v">

        <!-- Navigation Menu -->
        <nav id="navMenu" class="nav-menu">
          <ul class="menu">
            <li><a href="/index.html#home">Home</a></li>
            <li><a href="/index.html#about">About</a></li>
            <li><a href="/index.html#research">Research</a></li>
            <li><a href="/index.html#team">Team</a></li>
            <li><a href="/index.html#news">News</a></li>
          </ul>
        </nav>

      </div>

    </div>

    <!-- Site Header Banner -->
    <div class="row header-banner parent" data-scrollax="properties: { 'translateY': '48%', 'opacity': '1.4' }">

      <div class="heading-content text-center child-center-v">

        <!-- Heading Title -->
        <div class="row heading-title">
          <h2>
            
            
            
          </h2>
          <h2 class="thin">LiftPose3D, a deep learning-based approach for transforming two-dimensional to three-dimensional poses in laboratory animals</h2>
        </div>

      </div>

    </div>

    <!-- Arrow Down -->
    <div class="row arrow-down-section" data-scrollax="properties: { 'opacity': '0.6' }">

      <div class="wrapper-arrow child-center">
        <a href="#start" class="arrow-down">
          <i class="pe-7s-angle-down"></i>
        </a>
      </div>

    </div>

  </div>
</header>


    <!-- Page Section -->
    <section class="site-section blog-section page-layout" id="start">
      <div class="container">

        <!-- Page Layout -->
        <div class="row">

          <!-- Content -->
          <div class="col-md-8 blog-articles-container">

            <h2 id="reconstructing-3d-animal-poses-from-single-images">Reconstructing 3D animal poses from single images</h2>

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">Animal behaviours are vastly diverse, from dexterous leg movements of a fruit fly foraging for food or the acrobatic swing of a macaque between trees. To understand the neural basis of these behaviours, neuroscientists have long been pushing the frontiers to find ways to describe of these motions with increasing fidelity. Early techniques have relied on techniques such as <a href="https://en.wikipedia.org/wiki/Nikolai_Bernstein">cyclograms</a>, taken using long exposure photographs with stroboscopic lighting. However, technology has progressed immensely and modern neuroscience is increasingly relying on 3-dimensional pose tracking, i.e., by following the coordinates of a set of relevant body parts over time. 3D poses provide a complete description of a movement; using 3D poses the angle of any joint can be computed and any other description can be unambiguously derived. In addition, 3D poses are becoming increasingly relevant because they bridge the gap between the converging advances in biomechanics and robotics.</p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2 class="wp-block-heading">3D pose tracking has high hardware requirements</h2>
<!-- /wp:heading -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">Despite the advantages of 3D poses, obtaining them has required considerably effort and specialised hardware setups with multiple synchronised cameras keeping the moving animal in focus. This is because previous techniques of obtaining 3D poses relied on <a href="https://en.wikipedia.org/wiki/Triangulation_(computer_vision)">triangulation</a>, which is a method of inferring the 3D coordinate of a point based on its 2D location from multiple camera angles. A line drawn through a point in 3D space and the focal point of the camera has a unique projection on the camera plane (sensor). Further, two (non-coincidental) lines can only have one unique intersection. It follows that triangulating a point requires having at least two cameras and performing <a href="https://www.sciencedirect.com/topics/computer-science/camera-calibration">camera calibration</a>, in order to know the relative orientation of the cameras. To ensure that all points of interest in a moving animal are in focus of at least two, but preferably more cameras for increased accuracy, state-of-the-art <a href="https://elifesciences.org/articles/48571">fruit fly</a> and <a href="https://www.cell.com/neuron/pdf/S0896-6273(20)30894-1.pdf">rodent studies</a> studies have used six cameras, while a <a href="https://www.nature.com/articles/s41467-020-18441-5">recent study</a> in macaques used as many as 62 cameras!</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":733,"sizeSlug":"medium","linkDestination":"none","align":"center"} -->
<figure class="wp-block-image aligncenter size-medium is-resized"><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-05-at-21.38.12.png?w=300" alt="" class="wp-image-733" style="width:415px;height:278px" /><figcaption class="wp-element-caption"><strong>3D pose estimation has required many synchronised cameras in the past.</strong> Examples shows recording of tethered fruit flies using six cameras</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">Such hardware requirement pose substantial challenges particularly when studying animal behaviour in naturalistic environments, where the animal can be an any position relative to the cameras. As a result, some joints may be self-occluded (not visible from a given camera), which may mean that there are not enough camera views to perform triangulation. Therefore, many previous studies have used 2D poses or other metrics such as gait diagrams. However, these descriptors do not allow the unambiguous inference of 3D poses, they often lead to uncertainties in kinematic or behavioural analyses. </p>
<!-- /wp:paragraph -->

<!-- wp:heading -->
<h2 class="wp-block-heading">Lifting 2D poses to 3D poses</h2>
<!-- /wp:heading -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">Our approach, LiftPose3D, relies on a different technique called lifting, which surmounts the challenges associated with 3D pose estimation by reconstructing 3D poses directly from 2D poses. At first sight this may sound like a contradiction: if a 2D pose can correspond to multiple 3D poses, how can the 3D pose be reconstructed from a given 2D pose? This contradiction is resolved by realising that the pose repertoire of animals covers only a fraction of all possible poses. Indeed, owing to limits in the range of motion of joints and fact that animals seek to find efficient ways to coordinate their joints, the position of any limb is in a strict geometric relationship with other limbs. LiftPose3D uses a <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural net</a> to learn these spatial geometric relationships between key points using a 3D pose library of typically used poses. Effectively, the network acts as a regressor, which for a given 2D pose input infers the most likely 3D pose in the pose library. The network is <a href="https://en.wikipedia.org/wiki/Artificial_neural_network#Training">trained</a> via an extensive array of 2D-3D pose pairs that cover the typically used pose repertoire of the animal during its movement.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":425,"width":259,"height":201,"sizeSlug":"large","linkDestination":"none","align":"center"} -->
<figure class="wp-block-image aligncenter size-large is-resized"><img src="https://adamgosztolai.files.wordpress.com/2021/08/screenshot-2021-08-03-at-11.35.44-01-1.png?w=380" alt="" class="wp-image-425" style="width:259px;height:201px" /><figcaption class="wp-element-caption">The deep neural architecture used by LiftPose3D. For details, click <a href="https://www.nature.com/articles/s41592-021-01226-z#MOESM4">here</a>.</figcaption></figure>
<!-- /wp:image -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">Although the neural network that we use is relatively shallow (only contains 6 layers in total) it still requires a substantial amount of data be trained, that is often not available by experimentalists. LiftPose3D’s unique advantages lie in the data augmentation techniques it uses, which can overcome the challenges in training data often faced in laboratory studies associated with&nbsp;a wide range of animals, camera angles, experimental systems, and behaviours.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">Our study demonstrated in <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41592-021-01226-z/MediaObjects/41592_2021_1226_MOESM3_ESM.mp4">fruit flies</a> and <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41592-021-01226-z/MediaObjects/41592_2021_1226_MOESM6_ESM.mp4">macaques</a> that a library of 3D poses can be used to train our network to lift 2D poses from one camera, while not having to know the camera positioning. Consequently, no camera calibration is required and pre-trained LiftPose3D networks can be used across laboratories and datasets. Strikingly, the accuracy of 3D poses is almost as good as triangulation.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":403,"sizeSlug":"large","linkDestination":"none","align":"center"} -->
<figure class="wp-block-image aligncenter size-large"><img src="https://adamgosztolai.files.wordpress.com/2021/08/ezgif-2-41be6556cd9c.gif?w=600" alt="" class="wp-image-403" /><figcaption class="wp-element-caption"><strong>Obtaining 3D pose in tethered Drosophila</strong> (left) Tethering makes it easy to keep the animal in focus by multiple synchronised cameras. (right) Since multiple viewpoints are available for each joint, their positions can be triangulated.</figcaption></figure>
<!-- /wp:image -->

<!-- wp:heading -->
<h2 class="wp-block-heading">Occlusions</h2>
<!-- /wp:heading -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">As mentioned earlier, occlusions can be very common in freely moving animals because some body parts may be hidden from some cameras due obstacles or simply the position of the animal. These conditions often make triangulation impossible for occluded joints, hence only incomplete 3D poses can be obtained. A challenge here is that with such <em>incomplete</em> 3D poses it is not clear how to train a LiftPose3D network which can infer <em>complete</em> 3D poses from 2D poses. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">We found that by aligning the animals in the same coordinate frame and replacing the unknown coordinates by zeros the network could be efficiently trained to predict full 3D poses. In fact, alignment removed the uncertainty regarding the animals position in 3D space, thus the network only needs to learn the geometric relationships between keypoints in animal poses. Further, replacing the unseen coordinates by zeros means that occluded points do not bias the network training. We also showed in <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41592-021-01226-z/MediaObjects/41592_2021_1226_MOESM7_ESM.mp4">fruit flies</a>, <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41592-021-01226-z/MediaObjects/41592_2021_1226_MOESM8_ESM.mp4">mice</a> and <a href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41592-021-01226-z/MediaObjects/41592_2021_1226_MOESM9_ESM.mp4">rats</a> that our network can overcome occlusions and outliers in training data.</p>
<!-- /wp:paragraph -->

<!-- wp:image {"width":834,"height":237,"align":"center"} -->
<figure class="wp-block-image aligncenter is-resized"><img src="https://adamgosztolai.files.wordpress.com/2021/08/ezgif-2-e6aa750cc4f7-1.gif" alt="" style="width:834px;height:237px" /><figcaption class="wp-element-caption"><strong>Training the network with incomplete information.</strong> (left) The moving fruit fly is viewed from two angles, a bottom (ventral) view and a side (lateral) view. Depending on the orientation of the animal, some joints are not visible from the side view. In the video, line segments are drawn whenever the 2D pose detector could detect the leg segment. (right) Using the ventral view, where all joints are visible, LiftPose3D could predict the 3D position of the joints, even those that are not visible from the side view (dashed line). The predictions correspond closely to the true positions (solid line) when these are known. </figcaption></figure>
<!-- /wp:image -->

<!-- wp:heading -->
<h2 class="wp-block-heading">Adapting a LiftPose3D network to new experiments</h2>
<!-- /wp:heading -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">Finally, when 3D pose information is not available, for example, in experiments where only 2D data has been collected using a single camera, then LiftPose3D may still be used to obtain 3D poses. This scenario may not seem different from the above at first sight because it simply involves predicting 3D pose from 2D pose. Naively, a network could be trained on another dataset where 3D poses are available and applied directly to the 2D poses to predict 3D poses. However, different experiments can involve different animals having varying proportions and different cameras having different hardware-related distortions. Thus, it is unlikely that this naive approach will succeed because the network may not be able to interpret 2D poses given this unknown variability. Even if it could, it is unlikely to yield 3D poses with the desired accuracy.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">To account for the variability in the new dataset, we used pre-trained networks in combination with <a href="https://en.wikipedia.org/wiki/Domain_adaptation">domain adaptation</a> to generalise networks across datasets. In other words, we initially proceeded with the naive approach by training a network using another dataset where 3D poses were available. However, rather than applying the network to the 2D poses in the new dataset directly, we deformed these 2D poses to 'look like' those in the dataset in which the network was trained. We found that this approach could effectively remove small differences between datasets to lift realistic 3D poses. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph {"style":{"typography":{"lineHeight":1.6}}} -->
<p style="line-height:1.6">It has never been easier to obtain 3D poses of animals! The following image sequence shows images obtained using an open source setup we called LiftPose3D station, which we build from less than $150. We provide the hardware list, build instructions, 2D network, LiftPose3D network for your next <em>Drosophila</em> project!</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":400,"sizeSlug":"large","linkDestination":"none","align":"center"} -->
<figure class="wp-block-image aligncenter size-large"><img src="https://adamgosztolai.files.wordpress.com/2021/08/video_9.gif?w=1024" alt="" class="wp-image-400" /><figcaption class="wp-element-caption"><strong>Reconstructing 3D poses of Drosophila in the LiftPose3D station</strong> (left) Ventral video of a walking Drosophila. (middle) Cropped and aligned video superimposed with 2D poses annotated by a trained DeepLabCut network. (right) 3D poses reconstructed by LiftPose3D from the ventral 2D poses.</figcaption></figure>
<!-- /wp:image -->

<!-- wp:separator {"opacity":"css"} -->
<hr class="wp-block-separator has-css-opacity" />

<!-- /wp:separator -->

<!-- wp:heading -->
<h2 class="wp-block-heading">References</h2>
<!-- /wp:heading -->

<!-- wp:paragraph -->
<p><strong>Gosztolai, A</strong>., Günel, S., Rios, V.L., Abrate, M.P., Morales, D., Rhodin, H., Fua, P. and Ramdya, P. LiftPose3D, a deep learning-based approach for transforming 2D to 3D poses in laboratory animals.&nbsp;<em>Nature Methods</em>, <strong>18</strong>, 975–981 (2021) [<a rel="noreferrer noopener" href="http://biorxiv.org/lookup/doi/10.1101/2020.09.18.292680" target="_blank">article</a>] [<a rel="noreferrer noopener" href="https://github.com/NeLy-EPFL/LiftPose3D" target="_blank">code</a>][<a href="https://www.youtube.com/watch?v=0GTncPh-_bM">video summary</a>]</p>
<!-- /wp:paragraph -->


          </div>

        </div>

      </div>
    </section>

    <!-- Site Footer -->
<footer class="site-footer" id="footer">
  <div class="container">

    <div class="row">
      <div class="col-md-6 col-md-push-3 footer-info text-center">

        <div class="social-icons">
          <ul class="icons">
            <!-- <li><a href="" target="_blank"><i class="fi flaticon-youtube"></i></a></li> -->
            <li><a href="https://twitter.com/AGosztolai" target="_blank"><i class="fa-brands fa-square-x-twitter"></i></a></li>
            <li><a href="https://scholar.google.com/citations?user=fVGCjMsAAAAJ&hl=en" target="_blank"><i class="fa-brands fa-google-scholar"></i></a></li>
          </ul>
        </div>

      </div>

    </div>

  </div>
</footer>

<!-- Go To Top Button -->
<a href="#home" class="go-to-top-button"><i class="pe-7s-angle-up"></i></a>


    <!-- Importing Scripts -->

<!-- Fontawesome -->
<script src="https://kit.fontawesome.com/590872f71d.js" crossorigin="anonymous"></script>

<!-- jQuery CDN -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/3.0.0/jquery.min.js"></script>

<!-- If CDN fails than use local jQuery -->
<script>window.jQuery || document.write('<script src="/js/jquery-3.0.0.min.js"><\/script>')</script>

<!-- Owl Carousel -->
<script src="/js/carousel.min.js" charset="utf-8"></script>

<!-- Parallaxing & SmoothScroll -->
<script src="/js/scrollax.min.js" charset="utf-8"></script>



<!-- Main JS -->
<script src="/js/main.js" charset="utf-8"></script>

<script>

document.addEventListener("DOMContentLoaded", function() {
  let paths = document.querySelectorAll('.lines path');
  let polygonsGroup = document.querySelector('.polygons');
  let svgContainer = document.querySelector('.line-container');
  let completedAnimation = false;
  let animationDuration = 3000; // Animation duration in milliseconds
  let animationStartTime;
  let targetTime = animationDuration; // Target time for animation completion
  let animationDelay = 2000; // Delay in milliseconds before animation starts

  polygonsGroup.style.opacity = '0';

  paths.forEach((path) => {
    let pathLength = path.getTotalLength();
    path.style.strokeDasharray = pathLength + ' ' + pathLength;
    path.style.strokeDashoffset = pathLength;
  });

  function startAnimation() {
    animationStartTime = performance.now(); // Get the current timestamp
    requestAnimationFrame(animate);
  }

  function animate(currentTime) {
    let elapsed = currentTime - animationStartTime;
    let animationProgress = Math.min(elapsed / animationDuration, 1);

    paths.forEach((path) => {
      let pathLength = path.getTotalLength();
      let drawLength = pathLength * (1 - animationProgress);
      path.style.strokeDashoffset = drawLength;
    });

    if (animationProgress < 1) {
      requestAnimationFrame(animate);
    } else {
      polygonsGroup.style.opacity = '1';
    }
  }

  setTimeout(startAnimation, animationDelay); // Start the animation after a delay
});

  </script>


  </body>

</html>
